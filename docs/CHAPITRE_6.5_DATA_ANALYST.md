# CHAPITRE 6.5 : MODULE D'ANALYSE DE DONN√âES ET REPORTING

## Introduction

Dans le cadre de la formation **Bachelor Full Stack & Data Analyst**, l'aspect analyse de donn√©es constitue un pilier essentiel du projet. Ce chapitre pr√©sente le module d'analytics d√©velopp√© pour transformer les donn√©es brutes des dossiers de cr√©dit en informations exploitables, facilitant ainsi la prise de d√©cision strat√©gique au sein de la GGR du Cr√©dit du Congo.

Le module d'analytics r√©pond √† trois objectifs principaux :
1. **Visualisation des KPIs** : Tableaux de bord interactifs avec indicateurs cl√©s de performance
2. **Analyse statistique** : Rapports d√©taill√©s sur les tendances et performances
3. **Pr√©diction ML** : Mod√®le de machine learning pour l'√©valuation du risque cr√©dit

---

## 6.5.1. Architecture du Module Analytics

### A. Structure Modulaire

Le module `analytics` est organis√© selon une architecture en couches :

```
analytics/
‚îú‚îÄ‚îÄ models.py           # Mod√®les de donn√©es statistiques
‚îú‚îÄ‚îÄ services.py         # Logique m√©tier et calculs
‚îú‚îÄ‚îÄ views.py            # Contr√¥leurs et API
‚îú‚îÄ‚îÄ urls.py             # Routage
‚îú‚îÄ‚îÄ admin.py            # Interface d'administration
‚îî‚îÄ‚îÄ ml_models/          # Mod√®les ML sauvegard√©s
    ‚îú‚îÄ‚îÄ credit_risk_model.pkl
    ‚îî‚îÄ‚îÄ scaler.pkl
```

### B. Mod√®les de Donn√©es

Trois mod√®les principaux ont √©t√© cr√©√©s pour stocker les analyses :

#### 1. **StatistiquesDossier**
Agr√®ge les m√©triques globales par p√©riode (jour, semaine, mois, ann√©e) :
- Compteurs : total, en cours, approuv√©s, rejet√©s, archiv√©s
- Montants : total demand√©, total approuv√©, montant moyen
- D√©lais : temps moyen de traitement par √©tape
- Taux : approbation, rejet

#### 2. **PerformanceActeur**
√âvalue la performance individuelle des gestionnaires et analystes :
- Dossiers trait√©s par p√©riode
- Taux d'approbation personnel
- D√©lai moyen de traitement
- Score de performance calcul√©

#### 3. **PredictionRisque**
Stocke les pr√©dictions du mod√®le ML :
- Score de risque (0-100)
- Probabilit√© de d√©faut (0-1)
- Classification : FAIBLE, MOYEN, √âLEV√â
- Facteurs de risque identifi√©s
- Recommandation automatique

---

## 6.5.2. Dashboards Analytiques avec Charts.js

### A. Dashboard Principal

Le dashboard principal offre une vue d'ensemble avec :

**KPIs en temps r√©el** :
- üìÅ Total dossiers
- ‚è≥ Dossiers en cours
- ‚úÖ Taux d'approbation
- üìÖ Nouveaux dossiers du mois

**Graphiques interactifs** (Charts.js 4.4.0) :
1. **Graphique lin√©aire** : √âvolution mensuelle des dossiers (12 derniers mois)
2. **Graphique en donut** : R√©partition par statut
3. **Graphique en barres** : R√©partition par type de cr√©dit

### B. Impl√©mentation Technique

```javascript
// Exemple : Graphique d'√©volution mensuelle
const ctxEvolution = document.getElementById('chartEvolution').getContext('2d');
new Chart(ctxEvolution, {
    type: 'line',
    data: {
        labels: ['Jan', 'F√©v', 'Mar', ...],
        datasets: [{
            label: 'Nombre de dossiers',
            data: [12, 19, 15, ...],
            borderColor: '#667eea',
            backgroundColor: 'rgba(102, 126, 234, 0.1)',
            tension: 0.4,
        }]
    },
    options: {
        responsive: true,
        plugins: {
            legend: { position: 'top' },
            tooltip: { mode: 'index' }
        }
    }
});
```

### C. Service de Calcul Statistique

La classe `AnalyticsService` centralise tous les calculs :

```python
class AnalyticsService:
    @staticmethod
    def calculer_statistiques_periode(periode='MOIS'):
        # R√©cup√©ration des dossiers de la p√©riode
        dossiers = DossierCredit.objects.filter(created_at__gte=date_debut)
        
        # Calculs agr√©g√©s
        total = dossiers.count()
        approuves = dossiers.filter(statut_agent='APPROUVE').count()
        taux_approbation = (approuves / total * 100) if total > 0 else 0
        
        # Sauvegarde des statistiques
        stats = StatistiquesDossier.objects.create(...)
        return stats
```

---

## 6.5.3. Export Excel avec Statistiques (pandas)

### A. Fonctionnalit√© d'Export

Le module permet d'exporter les donn√©es en Excel avec deux feuilles :
1. **Feuille "Dossiers"** : Liste compl√®te des dossiers avec d√©tails
2. **Feuille "Statistiques"** : M√©triques agr√©g√©es

### B. Impl√©mentation avec pandas

```python
class ExportService:
    @staticmethod
    def exporter_statistiques_excel():
        # R√©cup√©ration des donn√©es
        dossiers = DossierCredit.objects.all().values(
            'reference', 'client__username', 'type_credit', 
            'montant_demande', 'statut_agent', 'created_at'
        )
        
        # Cr√©ation DataFrame pandas
        df = pd.DataFrame(list(dossiers))
        df.columns = ['R√©f√©rence', 'Client', 'Type', 'Montant', 'Statut', 'Date']
        
        # Statistiques agr√©g√©es
        stats = {
            'Total dossiers': [len(df)],
            'Montant total': [df['Montant'].sum()],
            'Montant moyen': [df['Montant'].mean()],
            'Taux approbation': [calcul_taux_approbation(df)]
        }
        df_stats = pd.DataFrame(stats)
        
        # Export Excel multi-feuilles
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Dossiers', index=False)
            df_stats.to_excel(writer, sheet_name='Statistiques', index=False)
        
        return filepath
```

### C. Avantages de l'Export Excel

- **Analyse hors ligne** : Les d√©cideurs peuvent analyser les donn√©es dans Excel
- **Pivot tables** : Cr√©ation de tableaux crois√©s dynamiques
- **Graphiques personnalis√©s** : Visualisations suppl√©mentaires
- **Archivage** : Conservation des rapports mensuels

---

## 6.5.4. Analyse Pr√©dictive avec Machine Learning

### A. Objectif du Mod√®le ML

Le mod√®le de machine learning pr√©dit le **risque de d√©faut de paiement** d'un dossier de cr√©dit avant sa validation finale, permettant aux analystes de :
- Identifier les dossiers √† risque √©lev√©
- Prioriser l'analyse approfondie
- R√©duire le taux de d√©faut

### B. Algorithme Utilis√© : Random Forest

**Choix de l'algorithme** :
- **Random Forest Classifier** (scikit-learn)
- Robuste aux donn√©es d√©s√©quilibr√©es
- Interpr√©table (importance des features)
- Performant sur des datasets de taille moyenne

### C. Features (Variables Pr√©dictives)

Le mod√®le utilise 6 features principales :
1. **Montant demand√©** : Montant du cr√©dit
2. **Dur√©e** : Dur√©e du pr√™t en mois
3. **Revenu mensuel** : Revenu du client
4. **Type de cr√©dit** : Immobilier, Consommation, Professionnel (encodage one-hot)

### D. Impl√©mentation

```python
class MLPredictionService:
    @staticmethod
    def entrainer_modele():
        # R√©cup√©ration des dossiers historiques
        dossiers = DossierCredit.objects.filter(
            statut_agent__in=['APPROUVE', 'REJETE']
        )
        
        # Pr√©paration des features
        X = []
        y = []
        for dossier in dossiers:
            features = [
                float(dossier.montant_demande),
                float(dossier.duree_mois),
                float(dossier.revenu_mensuel),
                1 if dossier.type_credit == 'IMMOBILIER' else 0,
                1 if dossier.type_credit == 'CONSOMMATION' else 0,
                1 if dossier.type_credit == 'PROFESSIONNEL' else 0,
            ]
            X.append(features)
            y.append(1 if dossier.statut_agent == 'REJETE' else 0)
        
        X = np.array(X)
        y = np.array(y)
        
        # Normalisation
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Entra√Ænement Random Forest
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_scaled, y)
        
        # Sauvegarde
        joblib.dump(model, 'analytics/ml_models/credit_risk_model.pkl')
        joblib.dump(scaler, 'analytics/ml_models/scaler.pkl')
        
        return model
    
    @staticmethod
    def predire_risque(dossier):
        # Chargement du mod√®le
        model = joblib.load('analytics/ml_models/credit_risk_model.pkl')
        scaler = joblib.load('analytics/ml_models/scaler.pkl')
        
        # Extraction features
        features = np.array([extraire_features(dossier)])
        features_scaled = scaler.transform(features)
        
        # Pr√©diction
        probabilite_defaut = model.predict_proba(features_scaled)[0][1]
        score_risque = probabilite_defaut * 100
        
        # Classification
        if score_risque < 30:
            classe_risque = 'FAIBLE'
            recommandation = "Approbation recommand√©e."
        elif score_risque < 60:
            classe_risque = 'MOYEN'
            recommandation = "Analyse approfondie recommand√©e."
        else:
            classe_risque = 'ELEVE'
            recommandation = "Prudence recommand√©e."
        
        # Sauvegarde de la pr√©diction
        prediction = PredictionRisque.objects.create(
            dossier=dossier,
            score_risque=score_risque,
            probabilite_defaut=probabilite_defaut,
            classe_risque=classe_risque,
            recommandation=recommandation,
        )
        
        return prediction
```

### E. Interpr√©tation des R√©sultats

**Score de risque** :
- **0-30** : Risque FAIBLE ‚Üí Approbation recommand√©e
- **30-60** : Risque MOYEN ‚Üí Analyse approfondie
- **60-100** : Risque √âLEV√â ‚Üí Prudence requise

**Facteurs de risque** :
Le mod√®le identifie les variables ayant le plus contribu√© au score (feature importance).

---

## 6.5.5. Interface Utilisateur Analytics

### A. Pages D√©velopp√©es

1. **`/analytics/dashboard/`** : Dashboard principal avec KPIs et graphiques
2. **`/analytics/rapport/`** : Rapport statistiques d√©taill√© par p√©riode
3. **`/analytics/predictions/`** : Liste des pr√©dictions ML
4. **`/analytics/export/excel/`** : T√©l√©chargement Excel

### B. Permissions d'Acc√®s

- **SUPER_ADMIN** : Acc√®s complet
- **RESPONSABLE_GGR** : Dashboards et rapports
- **ANALYSTE** : Pr√©dictions ML uniquement
- **GESTIONNAIRE** : KPIs basiques
- **CLIENT** : Aucun acc√®s (donn√©es sensibles)

---

## 6.5.6. Apports du Module Data Analyst

### A. Pour la Banque

1. **Prise de d√©cision √©clair√©e** :
   - Visualisation temps r√©el des performances
   - Identification des tendances
   - D√©tection des anomalies

2. **Optimisation op√©rationnelle** :
   - R√©duction du taux de d√©faut gr√¢ce au ML
   - Priorisation des dossiers √† risque
   - Am√©lioration des d√©lais de traitement

3. **Reporting automatis√©** :
   - G√©n√©ration automatique de rapports mensuels
   - Export Excel pour la direction
   - Tra√ßabilit√© des performances

### B. Pour le Projet Acad√©mique

1. **Comp√©tences Data Science** :
   - Manipulation de donn√©es avec pandas
   - Visualisation avec Charts.js
   - Machine Learning avec scikit-learn

2. **Conformit√© au dipl√¥me** :
   - Justification du titre "Full Stack & Data Analyst"
   - D√©monstration de comp√©tences analytiques
   - Application concr√®te du ML

---

## 6.5.7. Limites et Perspectives

### A. Limites Actuelles

1. **Mod√®le ML basique** :
   - Features limit√©es (6 variables)
   - Pas de validation crois√©e
   - Pas de tuning des hyperparam√®tres

2. **Donn√©es d'entra√Ænement** :
   - Dataset limit√© (projet acad√©mique)
   - Pas de donn√©es r√©elles sensibles

3. **Visualisations** :
   - Graphiques statiques (pas de drill-down)
   - Pas de filtres dynamiques avanc√©s

### B. Am√©liorations Futures

1. **Mod√®le ML avanc√©** :
   - Ajout de features (historique client, scoring externe)
   - Validation crois√©e et GridSearchCV
   - Comparaison d'algorithmes (XGBoost, LightGBM)
   - Explainability (SHAP values)

2. **Dashboards interactifs** :
   - Filtres dynamiques par p√©riode/acteur
   - Drill-down sur les graphiques
   - Alertes automatiques (seuils d√©pass√©s)

3. **Big Data** :
   - Int√©gration Apache Spark pour volumes importants
   - Data warehouse (PostgreSQL ‚Üí Redshift/BigQuery)
   - ETL automatis√©

4. **BI avanc√©** :
   - Int√©gration Power BI / Tableau
   - Rapports automatis√©s par email
   - Pr√©dictions en temps r√©el

---

## Conclusion du Chapitre 6.5

Le module d'analyse de donn√©es et reporting constitue une **valeur ajout√©e majeure** au projet Workflow GGR. Il transforme le syst√®me de gestion de dossiers en un **outil d'aide √† la d√©cision strat√©gique**, combinant :
- **Visualisation intuitive** (Charts.js)
- **Analyse statistique rigoureuse** (pandas)
- **Intelligence artificielle** (scikit-learn)

Ce module d√©montre la **double comp√©tence Full Stack & Data Analyst** de l'√©tudiante, en int√©grant harmonieusement le d√©veloppement web (Django) et l'analyse de donn√©es (Python Data Science stack).

Pour la banque, il repr√©sente un **levier d'optimisation** permettant de r√©duire les risques, d'am√©liorer les performances et de prendre des d√©cisions √©clair√©es bas√©es sur les donn√©es.

---

**Prochaine √©tape** : Chapitre 7 - Tests et Validation (incluant tests du module analytics)
